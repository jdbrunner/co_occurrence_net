\relax 
\citation{edge}
\citation{coocc}
\@writefile{toc}{\contentsline {section}{\numberline {1}Network Building}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Co-occurrence construction}{1}}
\citation{gut}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Correlation construction}{2}}
\citation{coocc}
\citation{gut}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Significance of the network}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The result of Monte-Carlo estimation of the likelihood of various network statistics, as well as the number of edges and mean degree. The networks were made from correlations using randomly drawn subsets of increasing size of the data at the species level, from 1 sample to all 60 samples. For each network, 1000 random trials were generated according to the above null model in order to estimate likelihood of statistics.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{montecarlos}{{1}{4}}
\newlabel{montecarlos@cref}{{[figure][1][]1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Networks Built}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Network Analysis}{4}}
\citation{PhysRevE.70.066111}
\citation{PhysRevE.70.056131}
\citation{vonLuxburg2007}
\citation{cyotscape}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Clustering of the correlation network using two methods compared to classification by sample metadata (location). Colors represent sample metadata. Nodes are grouped in circles by cluster.\relax }}{5}}
\newlabel{cluster}{{2}{5}}
\newlabel{cluster@cref}{{[figure][2][]2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Using the network to analyze a sample}{5}}
\citation{machine_learning}
\citation{machine_learning}
\citation{machine_learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A small network example\relax }}{6}}
\newlabel{toy_network}{{3}{6}}
\newlabel{toy_network@cref}{{[figure][3][]3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Determining $\psi _c$}{6}}
\citation{machine_learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Comparing configurations without specifying $\psi $}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Diffusion based method.}{8}}
\citation{vonLuxburg2007}
\citation{Anderson2010}
\newlabel{initalCond}{{1}{9}}
\newlabel{initalCond@cref}{{[method][1][]1}{9}}
\newlabel{first_wins}{{2}{9}}
\newlabel{first_wins@cref}{{[equation][2][]2}{9}}
\newlabel{boundarVal}{{2}{10}}
\newlabel{boundarVal@cref}{{[method][2][]2}{10}}
\newlabel{forcing}{{3}{11}}
\newlabel{forcing@cref}{{[method][3][]3}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Tests of these methods}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Diffusion is equivalent to the Kolmogorov forward equations for the above system.\relax }}{12}}
\newlabel{kfor}{{4}{12}}
\newlabel{kfor@cref}{{[figure][4][]4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Test networks. Gray nodes were ``unknown", blue nodes ``off", and red ``on".\relax }}{12}}
\newlabel{tests}{{5}{12}}
\newlabel{tests@cref}{{[figure][5][]5}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results of ranking nodes by likelihood of being ``on" by the three above methods\relax }}{12}}
\newlabel{rankres}{{1}{12}}
\newlabel{rankres@cref}{{[table][1][]1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Result of diffusion methods. Edge color represents the shared sample type of the nodes (i.e. samply type the taxa was found most often in), with gray indicating different types.\relax }}{13}}
\newlabel{diffusion_sample}{{6}{13}}
\newlabel{diffusion_sample@cref}{{[figure][6][]6}{13}}
\citation{Vishwanathan}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Diffusion Ranks\relax }}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of Networks}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Test networks. \relax }}{15}}
\newlabel{more_tests}{{8}{15}}
\newlabel{more_tests@cref}{{[figure][8][]8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Matching samples to networks}{15}}
\bibstyle{plain}
\bibdata{../../../summer17}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Results of sample assignment: $u_1$ cannot be decided, $u_2$ is assigned to network $A_1$, and $u_3$ is assigned to network $A_2$.\relax }}{16}}
\newlabel{tiny_res}{{9}{16}}
\newlabel{tiny_res@cref}{{[figure][9][]9}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A training data column fits the network better than a randomly generated sample.\relax }}{17}}
\newlabel{full_assign}{{10}{17}}
\newlabel{full_assign@cref}{{[figure][10][]10}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Fit scores for samples in networks. Random samples in (a) were samples in which 63 nodes were chosen to be present, uniformly, and given an abundance drawn from a $unif[0,0.1]$ distribution. In (b)'s random samples, I randomly choose the amount of non-zero values uniformly on the integers from 0 to the half a real sample length. I'll remake (a) like this.\relax }}{18}}
\newlabel{fit_scores}{{11}{18}}
\newlabel{fit_scores@cref}{{[figure][11][]11}{18}}
\bibcite{Anderson2010}{1}
\bibcite{PhysRevE.70.066111}{2}
\bibcite{gut}{3}
\bibcite{edge}{4}
\bibcite{machine_learning}{5}
\bibcite{coocc}{6}
\bibcite{PhysRevE.70.056131}{7}
\bibcite{cyotscape}{8}
\bibcite{Vishwanathan}{9}
\bibcite{vonLuxburg2007}{10}
